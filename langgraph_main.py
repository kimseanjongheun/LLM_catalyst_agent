"""
Langgraph ê¸°ë°˜ LLM Catalyst Agent Main Pipeline
ê¸°ì¡´ main.pyë¥¼ Langgraph í”„ë ˆì„ì›Œí¬ë¡œ ì¬êµ¬ì„±
"""

from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver
from agent.langgraph_nodes import (
    AgentState,
    load_context_node,
    prepare_search_group_node,
    generate_prompt_node,
    llm_inference_node,
    extract_compositions_node,  # ë³µìˆ˜í˜•ìœ¼ë¡œ ë³€ê²½
    extract_analysis_node,
    save_results_node,
    analyze_effectiveness_node,
    validate_results_node,
    error_handler_node
)


def should_continue_after_context(state: AgentState) -> str:
    """Context ë¡œë”© í›„ ë‹¤ìŒ ë‹¨ê³„ ê²°ì •"""
    if "error" in state and state["error"]:
        return "error_handler"
    return "continue"


def should_continue_after_search_group(state: AgentState) -> str:
    """Search group ì¤€ë¹„ í›„ ë‹¤ìŒ ë‹¨ê³„ ê²°ì •"""
    if "error" in state and state["error"]:
        return "error_handler"
    return "continue"


def should_continue_after_prompt(state: AgentState) -> str:
    """Prompt ìƒì„± í›„ ë‹¤ìŒ ë‹¨ê³„ ê²°ì •"""
    if "error" in state and state["error"]:
        return "error_handler"
    return "continue"


def should_continue_after_llm(state: AgentState) -> str:
    """LLM ì¶”ë¡  í›„ ë‹¤ìŒ ë‹¨ê³„ ê²°ì •"""
    if "error" in state and state["error"]:
        return "error_handler"
    return "continue"


def should_continue_after_extraction(state: AgentState) -> str:
    """ë‹¤ì¤‘ ì¡°ì„± ì¶”ì¶œ í›„ ë‹¤ìŒ ë‹¨ê³„ ê²°ì •"""
    if "error" in state and state["error"]:
        return "error_handler"
    return "continue"


def should_continue_after_analysis_extraction(state: AgentState) -> str:
    """ë¶„ì„ ì¶”ì¶œ í›„ ë‹¤ìŒ ë‹¨ê³„ ê²°ì •"""
    if "error" in state and state["error"]:
        return "error_handler"
    return "continue"


def should_continue_after_analysis(state: AgentState) -> str:
    """íš¨ê³¼ì„± ë¶„ì„ í›„ ë‹¤ìŒ ë‹¨ê³„ ê²°ì •"""
    if "error" in state and state["error"]:
        return "error_handler"
    return "continue"


def should_continue_after_validation(state: AgentState) -> str:
    """ê²°ê³¼ ê²€ì¦ í›„ ë‹¤ìŒ ë‹¨ê³„ ê²°ì •"""
    if "error" in state and state["error"]:
        return "error_handler"
    return "continue"


def create_agent_graph():
    """LLM Catalyst Agent ê·¸ë˜í”„ ìƒì„±"""
    
    # StateGraph ì´ˆê¸°í™”
    workflow = StateGraph(AgentState)
    
    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("load_context", load_context_node)
    workflow.add_node("prepare_search_group", prepare_search_group_node)
    workflow.add_node("generate_prompt", generate_prompt_node)
    workflow.add_node("llm_inference", llm_inference_node)
    workflow.add_node("extract_compositions", extract_compositions_node)  # ë³µìˆ˜í˜•
    workflow.add_node("extract_analysis", extract_analysis_node)
    workflow.add_node("analyze_effectiveness", analyze_effectiveness_node)
    workflow.add_node("validate_results", validate_results_node)
    workflow.add_node("save_results", save_results_node)
    workflow.add_node("error_handler", error_handler_node)
    
    # ì‹œì‘ì  ì„¤ì •
    workflow.set_entry_point("load_context")
    
    # ìˆœì°¨ì  ì‹¤í–‰ ê²½ë¡œ ì •ì˜
    workflow.add_conditional_edges(
        "load_context",
        should_continue_after_context,
        {
            "continue": "prepare_search_group",
            "error_handler": "error_handler"
        }
    )
    
    workflow.add_conditional_edges(
        "prepare_search_group", 
        should_continue_after_search_group,
        {
            "continue": "generate_prompt",
            "error_handler": "error_handler"
        }
    )
    
    workflow.add_conditional_edges(
        "generate_prompt",
        should_continue_after_prompt,
        {
            "continue": "llm_inference", 
            "error_handler": "error_handler"
        }
    )
    
    workflow.add_conditional_edges(
        "llm_inference",
        should_continue_after_llm,
        {
            "continue": "extract_compositions",  # ë³µìˆ˜í˜•
            "error_handler": "error_handler"
        }
    )
    
    workflow.add_conditional_edges(
        "extract_compositions",  # ë³µìˆ˜í˜•
        should_continue_after_extraction,
        {
            "continue": "extract_analysis",
            "error_handler": "error_handler"
        }
    )
    
    workflow.add_conditional_edges(
        "extract_analysis",
        should_continue_after_analysis_extraction,
        {
            "continue": "analyze_effectiveness",
            "error_handler": "error_handler"
        }
    )
    
    workflow.add_conditional_edges(
        "analyze_effectiveness",
        should_continue_after_analysis,
        {
            "continue": "validate_results",
            "error_handler": "error_handler"
        }
    )
    
    workflow.add_conditional_edges(
        "validate_results",
        should_continue_after_validation,
        {
            "continue": "save_results",
            "error_handler": "error_handler"
        }
    )
    
    # ê²°ê³¼ ì €ì¥ í›„ ì¢…ë£Œ
    workflow.add_edge("save_results", END)
    
    # ì˜¤ë¥˜ ì²˜ë¦¬ ê²½ë¡œ
    workflow.add_edge("error_handler", END)
    
    # ë©”ëª¨ë¦¬ ì €ì¥ê¸° ì„¤ì • (ì„ íƒì‚¬í•­)
    memory = MemorySaver()
    
    # ê·¸ë˜í”„ ì»´íŒŒì¼
    app = workflow.compile(checkpointer=memory)
    
    return app


def main():
    """Langgraph ê¸°ë°˜ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=== Langgraph ê¸°ë°˜ LLM Catalyst Agent ì‹œì‘ ===")
    print("ğŸ”§ OutputParser ì‹œìŠ¤í…œ ì ìš©ë¨")
    print("ğŸš€ ë‹¤ì¤‘ ì¡°ì„± ì¶”ì²œ ëª¨ë“œ í™œì„±í™”")
    
    try:
        # ê·¸ë˜í”„ ìƒì„±
        app = create_agent_graph()
        
        # ì´ˆê¸° ìƒíƒœ ì„¤ì •
        initial_state = {
            "context": {},
            "search_group": {},
            "prompt": "",
            "llm_output": "",
            "extracted_compositions": [],  # ë³µìˆ˜í˜• ë¦¬ìŠ¤íŠ¸
            "extracted_analysis": {},
            "tool_summary": {},
            "result": {},
            "timestamp": "",
            "error": ""
        }
        
        # ê·¸ë˜í”„ ì‹¤í–‰
        config = {"configurable": {"thread_id": "catalyst_agent_1"}}
        
        print("\nğŸš€ ê·¸ë˜í”„ ì‹¤í–‰ ì‹œì‘...")
        final_state = app.invoke(initial_state, config=config)
        
        print("\n=== ìµœì¢… ì‹¤í–‰ ê²°ê³¼ ===")
        if "error" in final_state and final_state["error"]:
            print(f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {final_state['error']}")
        else:
            print("âœ… ëª¨ë“  ë…¸ë“œê°€ ì„±ê³µì ìœ¼ë¡œ ì‹¤í–‰ë˜ì—ˆìŠµë‹ˆë‹¤!")
            print(f"ğŸ“ ê²°ê³¼ íŒŒì¼: results/latest_result.json")
            
            # OutputParser ê²°ê³¼ ìš”ì•½
            compositions = final_state.get("extracted_compositions", [])
            analysis = final_state.get("extracted_analysis", {})
            
            print(f"\nğŸ“Š ë‹¤ì¤‘ ì¡°ì„± OutputParser ê²°ê³¼:")
            print(f"  ğŸ§ª ì¶”ì¶œëœ ì¡°ì„± ê°œìˆ˜: {len(compositions)}")
            print(f"  ğŸ“ ë¶„ì„ ì¶”ì¶œ: {'âœ…' if analysis.get('analysis') else 'âŒ'}")
            print(f"  ğŸ’¡ ì¶”ì²œ ì¶”ì¶œ: {'âœ…' if analysis.get('recommendations') else 'âŒ'}")
            
            if compositions:
                print(f"\n  ğŸ”¬ ì¶”ì¶œëœ ì¡°ì„±ë“¤:")
                for i, comp in enumerate(compositions, 1):
                    print(f"    {i}. {comp}")
            
            if "tool_summary" in final_state:
                print(f"  ğŸ”§ MCP Tools ì‚¬ìš© íšŸìˆ˜: {final_state['tool_summary'].get('total_calls', 0)}")
        
        print("\n=== Langgraph ê¸°ë°˜ ë‹¤ì¤‘ ì¡°ì„± ì¶”ì²œ ì™„ë£Œ ===")
        
        return final_state
        
    except Exception as e:
        print(f"âŒ Langgraph ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None


def visualize_graph():
    """ê·¸ë˜í”„ êµ¬ì¡°ë¥¼ ì‹œê°í™” (ì„ íƒì‚¬í•­)"""
    try:
        app = create_agent_graph()
        
        # ê·¸ë˜í”„ êµ¬ì¡°ë¥¼ mermaid í˜•ì‹ìœ¼ë¡œ ì¶œë ¥
        print("\n=== ê·¸ë˜í”„ êµ¬ì¡° (Mermaid) ===")
        print("```mermaid")
        print("graph TD")
        print("    START --> load_context[Context ë¡œë”©]")
        print("    load_context --> prepare_search_group[Search Group ì¤€ë¹„]")
        print("    prepare_search_group --> generate_prompt[Prompt ìƒì„±]")
        print("    generate_prompt --> llm_inference[LLM ì¶”ë¡ ]")
        print("    llm_inference --> extract_compositions[ë‹¤ì¤‘ ì¡°ì„± ì¶”ì¶œ]")
        print("    extract_compositions --> extract_analysis[ë¶„ì„ ì¶”ì¶œ]")
        print("    extract_analysis --> analyze_effectiveness[íš¨ê³¼ì„± ë¶„ì„]")
        print("    analyze_effectiveness --> validate_results[ê²°ê³¼ ê²€ì¦]")
        print("    validate_results --> save_results[ê²°ê³¼ ì €ì¥]")
        print("    save_results --> END")
        print("    load_context -.-> error_handler[ì˜¤ë¥˜ ì²˜ë¦¬]")
        print("    prepare_search_group -.-> error_handler")
        print("    generate_prompt -.-> error_handler")
        print("    llm_inference -.-> error_handler")
        print("    extract_compositions -.-> error_handler")
        print("    extract_analysis -.-> error_handler")
        print("    analyze_effectiveness -.-> error_handler")
        print("    validate_results -.-> error_handler")
        print("    error_handler --> END")
        print("```")
        
    except Exception as e:
        print(f"ê·¸ë˜í”„ ì‹œê°í™” ì˜¤ë¥˜: {str(e)}")


if __name__ == "__main__":
    # ê·¸ë˜í”„ êµ¬ì¡° ì‹œê°í™” (ì„ íƒì‚¬í•­)
    # visualize_graph()
    
    # ë©”ì¸ ì‹¤í–‰
    main()